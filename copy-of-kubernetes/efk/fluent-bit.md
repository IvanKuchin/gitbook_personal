# Fluent-bit

## FILTER and PARSER

Deployment carefully described here: [https://docs.fluentbit.io/manual/installation/kubernetes](https://docs.fluentbit.io/manual/installation/kubernetes)

If log messages need to go through additional parser \[FILTER]->Merge\_Parser must be used (see  line (12))

```
    [FILTER]
        Name                kubernetes
        Match               kube.*_log-app-container-*
        Kube_URL            https://kubernetes.default.svc:443
        Kube_CA_File        /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        Kube_Token_File     /var/run/secrets/kubernetes.io/serviceaccount/token
        Kube_Tag_Prefix     kube.var.log.containers.
        Merge_Log           On
        Merge_Log_Key       log_processed
        K8S-Logging.Parser  On
        K8S-Logging.Exclude Off
        Merge_Parser        app_log
        
    [PARSER]
        Name        app_log
        Format      regex
        Regex       ^(?<datetime>[^\[]*)\[(?<pid>\d+)\](?<log_severity>(DEBUG|WARNING|ERROR|PANIC)):(?<pretty_function>[^\[]+)\[(?<line_number>\d+)\] *(?<log_message>.*)$
        Time_Key    time
        Time_Format %b %d %H:%M:%S
        Time_Keep   On
```

Line (2) - FILTER name `kubernetes`will enrich data with Kubernetes data (see output below)

Line(8), (9), (12) - tell to add log\_processed key-value pair to parsed response

Line (12), (14) - link between PARSER and FILTER sections

{% hint style="warning" %}
Do NOT use “-“ dash-sign in group-names in parser regex:
{% endhint %}

```
{
	"log": "Jul 10 00:27:58.475424[284] DEBUG: c_cache_obj::~c_cache_obj[81] cache hits = 0, cache misses = 1, cache performance = 0%\n",
	"stream": "stdout",
	"time": "2021-07-09T21:27:58.810455415Z",
	"log_processed": {
		"datetime": "Jul 10 00:27:58.475424",
		"pid": "284",
		"log_severity": "DEBUG",
		"pretty_function": " c_cache_obj::~c_cache_obj",
		"line_number": "81",
		"log_message": "cache hits = 0, cache misses = 1, cache performance = 0%"
	},
	"kubernetes": {
		"pod_name": "app-deployment-7d6c744896-h7n8p",
		"namespace_name": "www-connme-ru",
		"pod_id": "68d47fc8-7deb-4f82-a7bf-d7f5b85d2146",
		"labels": {
			"pod-template-hash": "7d6c744896",
			"type": "app"
		},
		"annotations": {
			"kubectl.kubernetes.io/restartedAt": "2021-06-29T17:35:07Z"
		},
		"host": "minikube",
		"container_name": "log-app-container",
		"docker_id": "9a1609ef670ee59f84afe025c813407dff92642a7b19277365add0f0cc1098f8",
		"container_hash": "busybox@sha256:930490f97e5b921535c153e0e7110d251134cc4b72bbb8133c6a5065cc68580d",
		"container_image": "busybox:latest"
	}
}

```

Lines (13) - (29) added by FILTER kubernetes

Lines (5) - (12) added by PARSER app\_log

## Troubleshooting

Fluent-bit container doesn’t have any shell

```
kubectl exec -it fluent-bit-xxxxxx -- /bin/sh (will fail)
kubectl exec -it fluent-bit-xxxxxx -- /bin/bash (will fail)
```

To get more visibility to logs generated by fluent-bit add \[OUTPUT]-folder as a hostmount in fluent-bit-configmap

```
  output-elasticsearch.conf: |
    [OUTPUT]
        Name            file
        Match           kube.*_log-app-container*
        Format          out_file
        Path            /log_output
```

Line (3) - instructs to save logs to file

Line (6) - configure directory inside fluent-bit container

Fluent-bit DaemonSet configuration. Just a normal hostmount.

```
spec: 
  volumes:
  - name: logoutput
    hostPath:
      path: /logoutput
  container:
    volumeMounts:
    - name: logoutput
      mountPath: /log_output

```

On the k8s-host there will be directory /logoutput with all fluent-bit log files in it.



## Log format (docker vs CRI-O)

According to [https://github.com/cri-o/cri-o/issues/3181](https://github.com/cri-o/cri-o/issues/3181) CRI-O prepend containers log message with: timestamp, output stream and stream tag. Example:&#x20;

```
2018-02-13T07:59:37.505201169-05:00 stdout F 2018-02-13 12:59:37 INFO  DefaultSearchGuardKeyStore:148 - sslTransport protocols [TLSv1.2, TLSv1.1]
2018-02-13T07:59:37.505201169-05:00 stdout F 2018-02-13 12:59:37 INFO  DefaultSearchGuardKeyStore:149 - sslHTTP protocols [TLSv1.2, TLSv1.1]
2018-02-13T07:59:41.839772246-05:00 stdout F 2018-02-13 12:59:41 INFO  transport:99 - [Christopher Summers] Using [com.floragunn.searchguard.ssl.transport.SearchGuardSSLNettyTransport] as transport, overridden by [search-guard-ssl]
2018-02-13T08:00:17.666563371-05:00 stdout F 2018-02-13 13:00:14 INFO  transport:420 - [Christopher Summers] failed to get node info for {#transport#-1}{127.0.0.1}{127.0.0.1:9300}, disconnecting...
2018-02-13T08:00:17.666563371-05:00 stdout F ReceiveTimeoutTransportException[[][127.0.0.1:9300][cluster:monitor/nodes/liveness] request_id [0] timed out after [5101ms]]
2018-02-13T08:00:17.666563371-05:00 stdout F    at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:698)
2018-02-13T08:00:17.666563371-05:00 stdout F    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2018-02-13T08:00:17.666563371-05:00 stdout F    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2018-02-13T08:00:17.666563371-05:00 stdout F    at java.lang.Thread.run(Thread.java:748)
2018-02-13T08:00:17.760141657-05:00 stdout F Contacting elasticsearch cluster 'elasticsearch' and wait for YELLOW clusterstate ...
```

While docker simply using whatever container has sent. Therefore different PARSERs configured for docker and CRI-O environments.

```
[PARSER]
    Name        app_log_docker
    Format      regex
    Regex       ^(?<datetime>[^\[]*)\[(?<pid>\d+)\] (?<log_severity>(DEBUG|INFO|WARNING|ERROR|PANIC)):(?<pretty_function>[^\[]+)\[(?<line_number>\d+)\] *(?<log_message>(?>chrono duration = (?<chrono_duration>\d+) microseconds|.*))$
    Types       chrono_duration:integer line_number:integer pid:integer
    Time_Key    time
    Time_Format %b %d %H:%M:%S
    Time_Keep   On

[PARSER]
    Name        app_log_crio
    Format      regex
    Regex       ^(?<crio_log_time>.+) (?<crio_log_stream>stdout|stderr) (?<crio_log_tag>F|P) (?<datetime>[^\[]*)\[(?<pid>\d+)\] (?<log_severity>(DEBUG|INFO|WARNING|ERROR|PANIC)):(?<pretty_function>[^\[]+)\[(?<line_number>\d+)\] *(?<log_message>(?>chrono duration = (?<chrono_duration>\d+) microseconds|.*))$
    Types       chrono_duration:integer line_number:integer pid:integer
    Time_Key    datetime
    Time_Format %b %d %Y %H:%M:%S.%L
    Time_Keep   On

```

Differences between the two CRI-O PARSER adds `crio_log_xxx` - specific values

## Milliseconds in regexp

Fluentbit uses strftime to parse timestamp: [https://www.cplusplus.com/reference/ctime/strftime/](https://www.cplusplus.com/reference/ctime/strftime/)

But milliseconds are not supported by it. Fluentbit added %L to support milliseconds. [https://github.com/fluent/fluent-bit/issues/703](https://github.com/fluent/fluent-bit/issues/703)

{% hint style="warning" %}
Note that subsecond resolution is a _hack_ since native strptime(3) don't support it.
{% endhint %}

## ISSUE: Failed to flush chunk

I've describe the issue here: [https://stackoverflow.com/questions/64958876/log-entries-lost-while-using-fluent-bit-with-kubernetes-filter-and-elasticsearch/70561174#70561174](https://stackoverflow.com/questions/64958876/log-entries-lost-while-using-fluent-bit-with-kubernetes-filter-and-elasticsearch/70561174#70561174)

During te problem some log messages has not appear in ES. By doing&#x20;

```
tail -f /var/log/containers/fluent<TAB>
```

Notice that fluent shows above error in case of the event. (I tried to get index of images folder https://\<DOMAIN>/images)

Then enable `Trace_Error on` in `[OUTPUT]` Fluentbit config. And get more detailed error description. In this case it was pid-field that conflicts with existing pid in index. Index-pid was long-type, while Fluentbit was sending string.&#x20;



